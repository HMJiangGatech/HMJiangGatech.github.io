<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Offline Policy Evaluation and a Case Study in Dialogue System :: Haoming Jiang</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Introduction of offline policy evaluation (ope)" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://hmjianggatech.github.io/notes/ope/" />




<link rel="stylesheet" href="https://hmjianggatech.github.io/assets/style.css">



<link rel="stylesheet" href="https://hmjianggatech.github.io/style.css">


<link rel="apple-touch-icon" href="https://hmjianggatech.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://hmjianggatech.github.io/favicon.ico">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Offline Policy Evaluation and a Case Study in Dialogue System">
<meta property="og:description" content="Introduction of offline policy evaluation (ope)" />
<meta property="og:url" content="https://hmjianggatech.github.io/notes/ope/" />
<meta property="og:site_name" content="Haoming Jiang" />

  
    <meta property="og:image" content="https://hmjianggatech.github.io/favicon.ico">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2020-06-09 17:00:11 -0400 EDT" />












</head>
<body class="orange">


<div class="container full headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Haoming Jiang
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/">About Me</a></li>
        
      
        
          <li><a href="/research">Research</a></li>
        
      
        
          <li><a href="https://arxiv.gtflashlab.com/">Arxiv Viewer</a></li>
        
      
        
          <li><a href="/projects">Projects</a></li>
        
      
        
          <li><a href="/news">News</a></li>
        
      
      
        <ul class="menu__sub-inner">
          <li class="menu__sub-inner-more-trigger">Show more ▾</li>

          <ul class="menu__sub-inner-more hidden">
            
              
                <li><a href="/notes">Notes</a></li>
              
            
              
                <li><a href="/others">Others</a></li>
              
            
              
                <li><a href="/gallery">Gallery</a></li>
              
            
          </ul>
        </ul>
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/">About Me</a></li>
      
    
      
        <li><a href="/research">Research</a></li>
      
    
      
        <li><a href="https://arxiv.gtflashlab.com/">Arxiv Viewer</a></li>
      
    
      
        <li><a href="/projects">Projects</a></li>
      
    
      
        <li><a href="/news">News</a></li>
      
    
      
        <li><a href="/notes">Notes</a></li>
      
    
      
        <li><a href="/others">Others</a></li>
      
    
      
        <li><a href="/gallery">Gallery</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://hmjianggatech.github.io/notes/ope/">Offline Policy Evaluation and a Case Study in Dialogue System</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2020-06-09 
      </span>
    
    
  </div>

  

  

  
    <div class="table-of-contents">
      <h2>
        
          Overview
        
      </h2>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#problem-setting">Problem Setting</a>
      <ul>
        <li><a href="#definition">Definition</a></li>
      </ul>
    </li>
    <li><a href="#two-approaches">Two approaches</a>
      <ul>
        <li><a href="#direct-method">Direct Method</a></li>
        <li><a href="#importance-sampling-is">Importance Sampling (IS)</a></li>
      </ul>
    </li>
    <li><a href="#doubly-robust-estimation">Doubly Robust Estimation</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
  

  <div class="post-content"><div>
        <h2 id="problem-setting">Problem Setting<a href="#problem-setting" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<blockquote>
<p><strong>Problem Definition</strong>: Evaluate a policy without interaction with environment.</p>
<p><strong>What do we have</strong>:<br>
<em>(i)</em> historical data \(\pi\): \( \mathcal{D}=\{s_i,a_i,r_i,s_i'\} \) (state, action, reward) pairs of one or more policies ( \( \mu_1, \mu_2, \mu_3, &hellip; \) ) interacting with environment.<br>
<em>(ii)</em> A target policy \(\pi\)
<em>(optional)</em> Expert policies \( \mu_1, \mu_2, \mu_3, &hellip; \) may or may not be accessible. In a more general setting, they are not accessible (e.g., human expert).</p>
</blockquote>
<h3 id="definition">Definition<a href="#definition" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><strong>OPE target</strong>:
$$ \rho(\pi) = \mathbb{E}_{(s,a,r) \sim d^\pi}(r) $$
We will often slightly abuse notation and write \( (s,a,r),(s,a),(s) \sim d^\pi \),\( d^\pi \) is the <em>normalized discounted stationary distribution</em> and is not not avaliable for target policy in OPE.</p>
<h2 id="two-approaches">Two approaches<a href="#two-approaches" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="direct-method">Direct Method<a href="#direct-method" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>DM directly estimate \( d^\pi \) by \( \widehat{d^\pi} \) and estimate reward by
$$ \hat\rho(\pi) = \mathbb{E}_{(s,a,r) \sim \widehat{d^\pi}}(r). $$
In dialogue, we can directly use self-play for this, where we also train a customer agent as the world model.</p>
<h3 id="importance-sampling-is">Importance Sampling (IS)<a href="#importance-sampling-is" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>IS (also called <em>inverse propensity score</em>) estimate reward by:sss</p>
<p>$$ \rho(\pi) = \mathbb{E}_{(s,a,r) \sim {d^\mu}}(r\frac{d^\pi (s,a)}{d^\mu (s,a)}) = \mathbb{E}_{(s,a,r) \sim {d^\mu}}(rw_{\pi/\mu}(s,a)), $$</p>
<p>where \(d^\pi (s,a)=d^\pi (s)\pi(a|s)\),  \(w_{\pi/\mu}(s,a)=\frac{d^\pi (s,a)}{d^\mu (s,a)}\) is the <em>discounted stationary distribution correction</em>.</p>
<p><strong>\(\mu\)  may be not available</strong>: 1.) Estimate old policy by \(\hat\mu\), which is not reliable. 2). Estimate \(w_{\pi/\mu}\)</p>
<h4 id="learning-correction">Learning Correction<a href="#learning-correction" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li><strong>TD Method</strong> (temporal difference) estimates the correction based on stationary distribution. TD method is very limited.</li>
<li><strong>Dual Dice</strong> Turn the problem into a minimax problem is a better approach for OPE.</li>
</ul>
<h2 id="doubly-robust-estimation">Doubly Robust Estimation<a href="#doubly-robust-estimation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h2 id="reference">Reference<a href="#reference" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>Doubly Robust Policy Evaluation and Learning</li>
<li>DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections</li>
</ul>

      </div></div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2021 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://hmjianggatech.github.io/assets/main.js"></script>
<script src="https://hmjianggatech.github.io/assets/prism.js"></script>




<script src="https://www.w3counter.com/tracker.js?id=129052"></script>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  
</div>

</body>
</html>
