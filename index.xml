<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Haoming Jiang (姜昊茗) on Haoming Jiang</title>
    <link>https://hmjianggatech.github.io/</link>
    <description>Recent content in Haoming Jiang (姜昊茗) on Haoming Jiang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Apr 2020 22:36:35 -0400</lastBuildDate><atom:link href="https://hmjianggatech.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My tool for Viewing arXiv Papers</title>
      <link>https://hmjianggatech.github.io/news/arxivviewer/</link>
      <pubDate>Wed, 24 Feb 2021 18:53:26 -0500</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/arxivviewer/</guid>
      <description>Recently, the hundreds of daily updates from arXiv make me feel overwhelmed. To help me better digest new machine learning papers, I developed an arXiv paper reading web app: https://arxiv.gtflashlab.com/ .
It gives me an Outlook-like user experience. It allows me to view the latest papers in an efficient, manageable, personalized way. It also provides some basic functions, that help me to manage these papers:
Reading status: it tells me whether I have viewed the paper or not.</description>
      <content>&lt;p&gt;Recently, the hundreds of daily updates from arXiv make me feel overwhelmed. To help me better digest new machine learning papers, I developed an arXiv paper reading web app: &lt;a href=&#34;https://arxiv.gtflashlab.com/&#34;&gt;https://arxiv.gtflashlab.com/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;It gives me an Outlook-like user experience. It allows me to view the latest papers in an efficient, manageable, personalized way.
It also provides some basic functions, that help me to manage these papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading status: it tells me whether I have viewed the paper or not.&lt;/li&gt;
&lt;li&gt;Archive: I can archive papers that I don&amp;rsquo;t want to see anymore.&lt;/li&gt;
&lt;li&gt;Star: Save my favorite papers.&lt;/li&gt;
&lt;li&gt;Tagging: it allows me to add tags to categorize the papers.&lt;/li&gt;
&lt;li&gt;Tagging from abstract/title: I can efficiently add tags from the title/abstract by simply selecting the text.&lt;/li&gt;
&lt;li&gt;Filtering by Reading status/Archive status/Star/Tags&lt;/li&gt;
&lt;li&gt;Read Abstract/PDF/HTML&lt;/li&gt;
&lt;li&gt;PDF Annotation which can be saved on the server.&lt;/li&gt;
&lt;li&gt;Search: Full-text-search on title and abstract (If you can&amp;rsquo;t find the paper, add &amp;ldquo;&amp;amp;&amp;amp;searchEngine=simple&amp;rdquo; to the URL. It will perform a case-insensitive exact-match search, which is kind of slow. Don&amp;rsquo;t blame me for the search experience, I am a newbie to the database. )
Citation/Reference (based on semantic scholar)&lt;/li&gt;
&lt;li&gt;View papers on your iPhone/iPad by saving the website as a bookmark on the homepage. (Although some things are still malfunctioning. /(ㄒoㄒ)/~~)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I want to share it with anyone who might find it useful for doing research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acknowledge&lt;/strong&gt;
It is powered by arXiv API, semantic scholar API, arXiv Vanity, and Adobe PDF API. This project is still in an early stage and I only import the machine learning related papers.
If you want to help me to improve it, visit my GitHub: &lt;a href=&#34;https://github.com/HMJiangGatech/ArxivRoller&#34;&gt;https://github.com/HMJiangGatech/ArxivRoller&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Our Adversarial Training Paper has been Accepted by AISTATS 2021</title>
      <link>https://hmjianggatech.github.io/news/aistats2021/</link>
      <pubDate>Sat, 23 Jan 2021 22:49:12 -0500</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/aistats2021/</guid>
      <description>Our Adversarial Training Paper has been Accepted by AISTATS 2021. We propose a learning-to-learn method to improve adversarial training on a variety of tasks.
Reference Learning to Defense by Learning to Attack 2021 Zhehui Chen*, Haoming Jiang*, Yuyang Shi, Bo Dai, and Tuo Zhao (* Equal Contribution) [arXiv] The 24th International Conference on Artificial Intelligence and Statistics (AISTATS), 2021 </description>
      <content>&lt;p&gt;Our Adversarial Training Paper has been Accepted by AISTATS 2021.
We propose a learning-to-learn method to improve adversarial training on a variety of tasks.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Learning to Defense by Learning to Attack&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2021&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Zhehui Chen*, Haoming Jiang*, Yuyang Shi, Bo Dai, and Tuo Zhao (* Equal Contribution)&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1811.01213.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)&lt;/em&gt;&lt;/strong&gt;, 2021

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Our BERT Calibration Paper has been Accepted by EMNLP 2020</title>
      <link>https://hmjianggatech.github.io/news/emnlp2020/</link>
      <pubDate>Wed, 23 Dec 2020 22:49:01 -0500</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/emnlp2020/</guid>
      <description>We propose to improve BERT model calibration via on-manifold smoothing and off-manifold smoothing.
Reference Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing 2020 Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao and Chao Zhang [arXiv] Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020 </description>
      <content>&lt;p&gt;We propose to improve BERT model calibration via on-manifold smoothing and off-manifold smoothing.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao and Chao Zhang&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2010.11506.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Hack WiC Leaderboard</title>
      <link>https://hmjianggatech.github.io/notes/secret_nmt/</link>
      <pubDate>Wed, 16 Sep 2020 23:50:07 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/notes/secret_nmt/</guid>
      <description> Some tricks for reproduce WMT EN-DE
WMT EN-DE
CUDA_VISIBLE_DEVICES=$GPUDEV fairseq-generate ../data-bin/wmt16_en_de_bpe32k \ --path $MODELDIR \ --batch-size 128 --beam 10 --lenpen 0.6 --remove-bpe \ --user-dir ../radam_fairseq &amp;gt; ${GEN} #GEN=raw_result SYS=$GEN.sys REF=$GEN.ref grep ^H $GEN | cut -f3- | perl -ple &amp;#39;s{(\S)-(\S)}{$1 ##AT##-##AT## $2}g&amp;#39; &amp;gt; $SYS grep ^T $GEN | cut -f2- | perl -ple &amp;#39;s{(\S)-(\S)}{$1 ##AT##-##AT## $2}g&amp;#39; &amp;gt; $REF fairseq-score --sys $SYS --ref $REF </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Some tricks for reproduce WMT EN-DE&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;WMT EN-DE&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=$GPUDEV fairseq-generate ../data-bin/wmt16_en_de_bpe32k \
--path $MODELDIR \
--batch-size 128 --beam 10 --lenpen 0.6 --remove-bpe \
--user-dir ../radam_fairseq &amp;gt; ${GEN}

#GEN=raw_result

SYS=$GEN.sys
REF=$GEN.ref

grep ^H $GEN | cut -f3- | perl -ple &amp;#39;s{(\S)-(\S)}{$1 ##AT##-##AT## $2}g&amp;#39; &amp;gt; $SYS
grep ^T $GEN | cut -f2- | perl -ple &amp;#39;s{(\S)-(\S)}{$1 ##AT##-##AT## $2}g&amp;#39; &amp;gt; $REF
fairseq-score --sys $SYS --ref $REF
&lt;/code&gt;&lt;/pre&gt;</content>
    </item>
    
    <item>
      <title>Offline Policy Evaluation and a Case Study in Dialogue System</title>
      <link>https://hmjianggatech.github.io/notes/ope/</link>
      <pubDate>Tue, 09 Jun 2020 17:00:11 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/notes/ope/</guid>
      <description>Problem Setting Problem Definition: Evaluate a policy without interaction with environment.
What do we have:
(i) historical data \(\pi\): \( \mathcal{D}=\{s_i,a_i,r_i,s_i&amp;rsquo;\} \) (state, action, reward) pairs of one or more policies ( \( \mu_1, \mu_2, \mu_3, &amp;hellip; \) ) interacting with environment. (ii) A target policy \(\pi\) (optional) Expert policies \( \mu_1, \mu_2, \mu_3, &amp;hellip; \) may or may not be accessible. In a more general setting, they are not accessible (e.</description>
      <content>&lt;h2 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem Definition&lt;/strong&gt;: Evaluate a policy without interaction with environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do we have&lt;/strong&gt;:&lt;br&gt;
&lt;em&gt;(i)&lt;/em&gt; historical data \(\pi\): \( \mathcal{D}=\{s_i,a_i,r_i,s_i&amp;rsquo;\} \) (state, action, reward) pairs of one or more policies ( \( \mu_1, \mu_2, \mu_3, &amp;hellip; \) ) interacting with environment. &lt;br&gt;
&lt;em&gt;(ii)&lt;/em&gt; A target policy \(\pi\)
&lt;em&gt;(optional)&lt;/em&gt; Expert policies \( \mu_1, \mu_2, \mu_3, &amp;hellip; \) may or may not be accessible. In a more general setting, they are not accessible (e.g., human expert).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;OPE target&lt;/strong&gt;:
$$ \rho(\pi) = \mathbb{E}_{(s,a,r) \sim d^\pi}(r) $$
We will often slightly abuse notation and write \( (s,a,r),(s,a),(s) \sim d^\pi \),\( d^\pi \) is the &lt;em&gt;normalized discounted stationary distribution&lt;/em&gt; and is not not avaliable for target policy in OPE.&lt;/p&gt;
&lt;h2 id=&#34;two-approaches&#34;&gt;Two approaches&lt;/h2&gt;
&lt;h3 id=&#34;direct-method&#34;&gt;Direct Method&lt;/h3&gt;
&lt;p&gt;DM directly estimate \( d^\pi \) by \( \widehat{d^\pi} \) and estimate reward by
$$ \hat\rho(\pi) = \mathbb{E}_{(s,a,r) \sim \widehat{d^\pi}}(r). $$
In dialogue, we can directly use self-play for this, where we also train a customer agent as the world model.&lt;/p&gt;
&lt;h3 id=&#34;importance-sampling-is&#34;&gt;Importance Sampling (IS)&lt;/h3&gt;
&lt;p&gt;IS (also called &lt;em&gt;inverse propensity score&lt;/em&gt;) estimate reward by:sss&lt;/p&gt;
&lt;p&gt;$$ \rho(\pi) = \mathbb{E}_{(s,a,r) \sim {d^\mu}}(r\frac{d^\pi (s,a)}{d^\mu (s,a)}) = \mathbb{E}_{(s,a,r) \sim {d^\mu}}(rw_{\pi/\mu}(s,a)), $$&lt;/p&gt;
&lt;p&gt;where \(d^\pi (s,a)=d^\pi (s)\pi(a|s)\),  \(w_{\pi/\mu}(s,a)=\frac{d^\pi (s,a)}{d^\mu (s,a)}\) is the &lt;em&gt;discounted stationary distribution correction&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;\(\mu\)  may be not available&lt;/strong&gt;: 1.) Estimate old policy by \(\hat\mu\), which is not reliable. 2). Estimate \(w_{\pi/\mu}\)&lt;/p&gt;
&lt;h4 id=&#34;learning-correction&#34;&gt;Learning Correction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TD Method&lt;/strong&gt; (temporal difference) estimates the correction based on stationary distribution. TD method is very limited.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dual Dice&lt;/strong&gt; Turn the problem into a minimax problem is a better approach for OPE.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;doubly-robust-estimation&#34;&gt;Doubly Robust Estimation&lt;/h2&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Doubly Robust Policy Evaluation and Learning&lt;/li&gt;
&lt;li&gt;DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Two Papers are Accepted by ICML 2020</title>
      <link>https://hmjianggatech.github.io/news/icml2020/</link>
      <pubDate>Sun, 07 Jun 2020 22:49:01 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/icml2020/</guid>
      <description> Two papers are accepted by ICML 2020!
One is about transformer-based point process model.
The other one is about using smooth policy to improve reinforcement learning in continuous environment.
Reference Deep Reinforcement Learning with Smooth Policy 2020 Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, Tuo Zhao [arXiv] International Conference on Machine Learning (ICML), 2020 Transformer Hawkes Process 2020 Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha [arXiv] International Conference on Machine Learning (ICML), 2020 </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ICML 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about transformer-based point process model.&lt;/p&gt;
&lt;p&gt;The other one is about using smooth policy to improve reinforcement learning in continuous environment.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Deep Reinforcement Learning with Smooth Policy&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2003.09534.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Transformer Hawkes Process&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2002.09291.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Google Cloud CheatSheet</title>
      <link>https://hmjianggatech.github.io/notes/google_cloud/</link>
      <pubDate>Mon, 18 May 2020 15:02:12 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/notes/google_cloud/</guid>
      <description>See more in link
Step 1: install nd setup gcloud SDK tutorial
Step 2: create vm export IMAGE_FAMILY=&amp;#34;tf-latest-cu92&amp;#34; export ZONE=&amp;#34;us-west1-b&amp;#34; export INSTANCE_NAME=&amp;#34;my-new-instance&amp;#34; export INSTANCE_TYPE=&amp;#34;n1-standard-8&amp;#34; export DISKSIZE=&amp;#34;120GB&amp;#34; gcloud compute instances create $INSTANCE_NAME \ --zone=$ZONE \ --image-family=$IMAGE_FAMILY \ --image-project=deeplearning-platform-release \ --maintenance-policy=TERMINATE \ --accelerator=&amp;#34;type=nvidia-tesla-v100,count=8&amp;#34; \ --machine-type=$INSTANCE_TYPE \ --boot-disk-size=$DISKSIZE \ --metadata=&amp;#34;install-nvidia-driver=True&amp;#34; Step 3: control your vm list vm: gcloud compute instances list vm status gcloud compute instances describe $INSTANCE_NAME access vm gcloud compute ssh $INSTANCE_NAME stop vm gcloud compute instances stop $INSTANCE_NAME start vm gcloud compute instances start $INSTANCE_NAME delete vm gcloud compute instances delete $INSTANCE_NAME </description>
      <content>&lt;p&gt;See more in &lt;a href=&#34;https://cloud.google.com/ai-platform/deep-learning-vm/docs/tensorflow_start_instance&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-1-install-nd-setup-gcloud-sdk&#34;&gt;Step 1: install nd setup gcloud SDK&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/gcloud-compute&#34;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-2-create-vm&#34;&gt;Step 2: create vm&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export IMAGE_FAMILY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tf-latest-cu92&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export ZONE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;us-west1-b&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export INSTANCE_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-new-instance&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export INSTANCE_TYPE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;n1-standard-8&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export DISKSIZE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;120GB&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances create $INSTANCE_NAME &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --zone&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$ZONE &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --image-family&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$IMAGE_FAMILY &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --image-project&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;deeplearning-platform-release &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --maintenance-policy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;TERMINATE &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --accelerator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type=nvidia-tesla-v100,count=8&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --machine-type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$INSTANCE_TYPE &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --boot-disk-size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$DISKSIZE &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        --metadata&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;install-nvidia-driver=True&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-3-control-your-vm&#34;&gt;Step 3: control your vm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;list vm:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances list
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;vm status&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances describe $INSTANCE_NAME
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;access vm&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute ssh $INSTANCE_NAME
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;stop vm&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances stop $INSTANCE_NAME
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;start vm&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances start $INSTANCE_NAME
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;delete vm&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud compute instances delete $INSTANCE_NAME
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Docker Cheatsheet</title>
      <link>https://hmjianggatech.github.io/notes/docker/</link>
      <pubDate>Sat, 16 May 2020 00:15:48 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/notes/docker/</guid>
      <description>Docker Introduction from docker.com
docker run -it --rm \ -v ~/project/MNLI:/work/MNLI \ -v ~/.ssh:/root/.ssh \ -v /mnt:/work/data \ --privileged docker-image:tag bash Run a docker image docker run -it --rm -v /local_dir:/docker_dir docker-image:tag bash -it: interactive mode --rm: clean up after exit -v: volume (shared filesystems) Attach to a running docker container docker ps docker exec -it container_name bash File Permission Issue All files created by the docker is owned by root.</description>
      <content>&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/&#34;&gt;Introduction from docker.com&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -it --rm &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-v ~/project/MNLI:/work/MNLI &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-v ~/.ssh:/root/.ssh &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-v /mnt:/work/data &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--privileged docker-image:tag bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;run-a-docker-image&#34;&gt;Run a docker image&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -it --rm  -v /local_dir:/docker_dir docker-image:tag bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;&lt;code&gt;-it&lt;/code&gt;: interactive mode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--rm&lt;/code&gt;: clean up after exit&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-v&lt;/code&gt;: volume (shared filesystems)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;attach-to-a-running-docker-container&#34;&gt;Attach to a running docker container&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker exec -it container_name bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;file-permission-issue&#34;&gt;File Permission Issue&lt;/h4&gt;
&lt;p&gt;All files created by the docker is owned by root.
Solution: &lt;a href=&#34;https://www.berthon.eu/2018/containers-volumes-and-file-permissions/&#34;&gt;Using ACL&lt;/a&gt;
The following command grant use ACL to make read/write permissions of all files in &lt;code&gt;project&lt;/code&gt; inherit from project and have the same access of the current host user.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;setfacl -dm &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;u:username:rwx&amp;#34;&lt;/span&gt; ~/project
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;ssh-in-docker&#34;&gt;SSH in docker&lt;/h4&gt;
&lt;p&gt;Add this line to use the ssh key as the host&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-v ~/.ssh:/root/.ssh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;build-a-docker-image&#34;&gt;Build a docker image&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Get folder with everything you need in it.&lt;/li&gt;
&lt;li&gt;Write a &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker build --tag=docker-image .&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;cheatsheet&#34;&gt;Cheatsheet&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## List Docker CLI commands&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker container --help
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Display Docker version and info&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker --version
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker version
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker info
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Execute Docker image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run hello-world
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## List Docker images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker image ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## List Docker containers (running, all, all in quiet mode)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker container ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker container ls --all
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker container ls -aq
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>BOND is Accepted by KDD 2020</title>
      <link>https://hmjianggatech.github.io/news/bond/</link>
      <pubDate>Fri, 15 May 2020 22:54:07 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/bond/</guid>
      <description> Our recent work on distantly supervised NER is accepted by KDD2020. We are closing the GAP between full supervision and no supervision! See our code here: https://github.com/cliang1453/BOND
The reuslts (F1 score) are summerized as follows:
Method CoNLL03 Tweet OntoNote5.0 Webpage Wikigold Full Supervision 91.21 52.19 86.20 72.39 86.43 Previous SOTA 76.00 26.10 67.69 51.39 47.54 BOND 81.48 48.01 68.35 65.74 60.07 Full Supervision: Roberta Finetuning/BiLSTM CRF Previous SOTA: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET Reference BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision 2020 Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution) The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2020 </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent work on distantly supervised NER is accepted by KDD2020.
We are closing the GAP between full supervision and no supervision!
See our code here: &lt;a href=&#34;https://github.com/cliang1453/BOND&#34;&gt;https://github.com/cliang1453/BOND&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts (F1 score) are summerized as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;CoNLL03&lt;/th&gt;
&lt;th&gt;Tweet&lt;/th&gt;
&lt;th&gt;OntoNote5.0&lt;/th&gt;
&lt;th&gt;Webpage&lt;/th&gt;
&lt;th&gt;Wikigold&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Full Supervision&lt;/td&gt;
&lt;td&gt;91.21&lt;/td&gt;
&lt;td&gt;52.19&lt;/td&gt;
&lt;td&gt;86.20&lt;/td&gt;
&lt;td&gt;72.39&lt;/td&gt;
&lt;td&gt;86.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Previous SOTA&lt;/td&gt;
&lt;td&gt;76.00&lt;/td&gt;
&lt;td&gt;26.10&lt;/td&gt;
&lt;td&gt;67.69&lt;/td&gt;
&lt;td&gt;51.39&lt;/td&gt;
&lt;td&gt;47.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BOND&lt;/td&gt;
&lt;td&gt;81.48&lt;/td&gt;
&lt;td&gt;48.01&lt;/td&gt;
&lt;td&gt;68.35&lt;/td&gt;
&lt;td&gt;65.74&lt;/td&gt;
&lt;td&gt;60.07&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Full Supervision&lt;/em&gt;: Roberta Finetuning/BiLSTM CRF&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution)&lt;/span&gt;





 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Two Papers are Accepted by ACL 2020</title>
      <link>https://hmjianggatech.github.io/news/acl2020/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/acl2020/</guid>
      <description> Two papers are accepted by ACL 2020!
One is about multidomain NMT using word level mixing model.
The other one is SMART model, leaderboad wining model on GLUE.
Reference SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization 2020 Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao [arXiv] [Code] Annual Conference of the Association for Computational Linguistics (ACL), 2020 Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing 2020 Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao [arXiv] Annual Conference of the Association for Computational Linguistics (ACL), 2020 </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ACL 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about multidomain NMT using word level mixing model.&lt;/p&gt;
&lt;p&gt;The other one is &lt;strong&gt;SMART&lt;/strong&gt; model, leaderboad wining model on GLUE.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.02692.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>State-of-the-art Results on GLUE Benchmark</title>
      <link>https://hmjianggatech.github.io/news/smart/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/smart/</guid>
      <description>Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (paper, code) achieves state-of-the-art results in 5 of 9 GLUE benchmark tasks and an overall GLUE task performance 89.9, which outperforms all existing models.
The reuslts are summerized as follows:
Method CoLA SST MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score #params Previous SOTA 70.8 97.1 91.9/89.2 92.5/92.1 74.6/90.4 92.0/91.7 96.7 92.5 93.2 53.1 89.7 11,000M MT-DNN-SMART 69.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (&lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;code&lt;/a&gt;) achieves state-of-the-art results in 5 of 9 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;GLUE benchmark tasks&lt;/a&gt; and an overall GLUE task performance &lt;strong&gt;89.9&lt;/strong&gt;, which outperforms all existing models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts are summerized as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;CoLA&lt;/th&gt;
&lt;th&gt;SST&lt;/th&gt;
&lt;th&gt;MRPC&lt;/th&gt;
&lt;th&gt;STS-B&lt;/th&gt;
&lt;th&gt;QQP&lt;/th&gt;
&lt;th&gt;MNLI-m/mm&lt;/th&gt;
&lt;th&gt;QNLI&lt;/th&gt;
&lt;th&gt;RTE&lt;/th&gt;
&lt;th&gt;WNLI&lt;/th&gt;
&lt;th&gt;AX&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;#params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Previous SOTA&lt;/td&gt;
&lt;td&gt;70.8&lt;/td&gt;
&lt;td&gt;97.1&lt;/td&gt;
&lt;td&gt;91.9/89.2&lt;/td&gt;
&lt;td&gt;92.5/92.1&lt;/td&gt;
&lt;td&gt;74.6/90.4&lt;/td&gt;
&lt;td&gt;92.0/91.7&lt;/td&gt;
&lt;td&gt;96.7&lt;/td&gt;
&lt;td&gt;92.5&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;53.1&lt;/td&gt;
&lt;td&gt;89.7&lt;/td&gt;
&lt;td&gt;11,000M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MT-DNN-SMART&lt;/td&gt;
&lt;td&gt;69.5&lt;/td&gt;
&lt;td&gt;97.5&lt;/td&gt;
&lt;td&gt;93.7/91.6&lt;/td&gt;
&lt;td&gt;92.9/92.5&lt;/td&gt;
&lt;td&gt;73.9/90.2&lt;/td&gt;
&lt;td&gt;91.0/90.8&lt;/td&gt;
&lt;td&gt;99.2&lt;/td&gt;
&lt;td&gt;89.7&lt;/td&gt;
&lt;td&gt;94.5&lt;/td&gt;
&lt;td&gt;50.2&lt;/td&gt;
&lt;td&gt;89.9&lt;/td&gt;
&lt;td&gt;356M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: T5&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
