<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>News on Haoming Jiang</title>
    <link>http://localhost:1313/news/</link>
    <description>Recent content in News on Haoming Jiang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 24 Feb 2021 18:53:26 -0500</lastBuildDate><atom:link href="http://localhost:1313/news/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My tool for Viewing arXiv Papers</title>
      <link>http://localhost:1313/news/arxivviewer/</link>
      <pubDate>Wed, 24 Feb 2021 18:53:26 -0500</pubDate>
      
      <guid>http://localhost:1313/news/arxivviewer/</guid>
      <description>&lt;p&gt;Recently, the hundreds of daily updates from arXiv make me feel overwhelmed. To help me better digest new machine learning papers, I developed an arXiv paper reading web app: &lt;a href=&#34;https://arxiv.gtflashlab.com/&#34;&gt;https://arxiv.gtflashlab.com/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;It gives me an Outlook-like user experience. It allows me to view the latest papers in an efficient, manageable, personalized way.
It also provides some basic functions, that help me to manage these papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading status: it tells me whether I have viewed the paper or not.&lt;/li&gt;
&lt;li&gt;Archive: I can archive papers that I don&amp;rsquo;t want to see anymore.&lt;/li&gt;
&lt;li&gt;Star: Save my favorite papers.&lt;/li&gt;
&lt;li&gt;Tagging: it allows me to add tags to categorize the papers.&lt;/li&gt;
&lt;li&gt;Tagging from abstract/title: I can efficiently add tags from the title/abstract by simply selecting the text.&lt;/li&gt;
&lt;li&gt;Filtering by Reading status/Archive status/Star/Tags&lt;/li&gt;
&lt;li&gt;Read Abstract/PDF/HTML&lt;/li&gt;
&lt;li&gt;PDF Annotation which can be saved on the server.&lt;/li&gt;
&lt;li&gt;Search: Full-text-search on title and abstract (If you can&amp;rsquo;t find the paper, add &amp;ldquo;&amp;amp;&amp;amp;searchEngine=simple&amp;rdquo; to the URL. It will perform a case-insensitive exact-match search, which is kind of slow. Don&amp;rsquo;t blame me for the search experience, I am a newbie to the database. )
Citation/Reference (based on semantic scholar)&lt;/li&gt;
&lt;li&gt;View papers on your iPhone/iPad by saving the website as a bookmark on the homepage. (Although some things are still malfunctioning. /(ㄒoㄒ)/~~)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I want to share it with anyone who might find it useful for doing research.&lt;/p&gt;</description>
      <content>&lt;p&gt;Recently, the hundreds of daily updates from arXiv make me feel overwhelmed. To help me better digest new machine learning papers, I developed an arXiv paper reading web app: &lt;a href=&#34;https://arxiv.gtflashlab.com/&#34;&gt;https://arxiv.gtflashlab.com/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;It gives me an Outlook-like user experience. It allows me to view the latest papers in an efficient, manageable, personalized way.
It also provides some basic functions, that help me to manage these papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading status: it tells me whether I have viewed the paper or not.&lt;/li&gt;
&lt;li&gt;Archive: I can archive papers that I don&amp;rsquo;t want to see anymore.&lt;/li&gt;
&lt;li&gt;Star: Save my favorite papers.&lt;/li&gt;
&lt;li&gt;Tagging: it allows me to add tags to categorize the papers.&lt;/li&gt;
&lt;li&gt;Tagging from abstract/title: I can efficiently add tags from the title/abstract by simply selecting the text.&lt;/li&gt;
&lt;li&gt;Filtering by Reading status/Archive status/Star/Tags&lt;/li&gt;
&lt;li&gt;Read Abstract/PDF/HTML&lt;/li&gt;
&lt;li&gt;PDF Annotation which can be saved on the server.&lt;/li&gt;
&lt;li&gt;Search: Full-text-search on title and abstract (If you can&amp;rsquo;t find the paper, add &amp;ldquo;&amp;amp;&amp;amp;searchEngine=simple&amp;rdquo; to the URL. It will perform a case-insensitive exact-match search, which is kind of slow. Don&amp;rsquo;t blame me for the search experience, I am a newbie to the database. )
Citation/Reference (based on semantic scholar)&lt;/li&gt;
&lt;li&gt;View papers on your iPhone/iPad by saving the website as a bookmark on the homepage. (Although some things are still malfunctioning. /(ㄒoㄒ)/~~)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I want to share it with anyone who might find it useful for doing research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acknowledge&lt;/strong&gt;
It is powered by arXiv API, semantic scholar API, arXiv Vanity, and Adobe PDF API. This project is still in an early stage and I only import the machine learning related papers.
If you want to help me to improve it, visit my GitHub: &lt;a href=&#34;https://github.com/HMJiangGatech/ArxivRoller&#34;&gt;https://github.com/HMJiangGatech/ArxivRoller&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Our Adversarial Training Paper has been Accepted by AISTATS 2021</title>
      <link>http://localhost:1313/news/aistats2021/</link>
      <pubDate>Sat, 23 Jan 2021 22:49:12 -0500</pubDate>
      
      <guid>http://localhost:1313/news/aistats2021/</guid>
      <description>&lt;p&gt;Our Adversarial Training Paper has been Accepted by AISTATS 2021.
We propose a learning-to-learn method to improve adversarial training on a variety of tasks.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Learning to Defense by Learning to Attack&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2021&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Zhehui Chen*, Haoming Jiang*, Yuyang Shi, Bo Dai, and Tuo Zhao (* Equal Contribution)&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1811.01213.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)&lt;/em&gt;&lt;/strong&gt;, 2021


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content>&lt;p&gt;Our Adversarial Training Paper has been Accepted by AISTATS 2021.
We propose a learning-to-learn method to improve adversarial training on a variety of tasks.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Learning to Defense by Learning to Attack&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2021&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Zhehui Chen*, Haoming Jiang*, Yuyang Shi, Bo Dai, and Tuo Zhao (* Equal Contribution)&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1811.01213.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)&lt;/em&gt;&lt;/strong&gt;, 2021


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Our BERT Calibration Paper has been Accepted by EMNLP 2020</title>
      <link>http://localhost:1313/news/emnlp2020/</link>
      <pubDate>Wed, 23 Dec 2020 22:49:01 -0500</pubDate>
      
      <guid>http://localhost:1313/news/emnlp2020/</guid>
      <description>&lt;p&gt;We propose to improve BERT model calibration via on-manifold smoothing and off-manifold smoothing.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao and Chao Zhang&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2010.11506.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content>&lt;p&gt;We propose to improve BERT model calibration via on-manifold smoothing and off-manifold smoothing.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao and Chao Zhang&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2010.11506.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Two Papers are Accepted by ICML 2020</title>
      <link>http://localhost:1313/news/icml2020/</link>
      <pubDate>Sun, 07 Jun 2020 22:49:01 -0400</pubDate>
      
      <guid>http://localhost:1313/news/icml2020/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ICML 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about transformer-based point process model.&lt;/p&gt;
&lt;p&gt;The other one is about using smooth policy to improve reinforcement learning in continuous environment.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Deep Reinforcement Learning with Smooth Policy&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2003.09534.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Transformer Hawkes Process&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2002.09291.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ICML 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about transformer-based point process model.&lt;/p&gt;
&lt;p&gt;The other one is about using smooth policy to improve reinforcement learning in continuous environment.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Deep Reinforcement Learning with Smooth Policy&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2003.09534.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Transformer Hawkes Process&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/2002.09291.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>BOND is Accepted by KDD 2020</title>
      <link>http://localhost:1313/news/bond/</link>
      <pubDate>Fri, 15 May 2020 22:54:07 -0400</pubDate>
      
      <guid>http://localhost:1313/news/bond/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Our recent work on distantly supervised NER is accepted by KDD2020.
We are closing the GAP between full supervision and no supervision!
See our code here: &lt;a href=&#34;https://github.com/cliang1453/BOND&#34;&gt;https://github.com/cliang1453/BOND&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts (F1 score) are summerized as follows:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Method&lt;/th&gt;
          &lt;th&gt;CoNLL03&lt;/th&gt;
          &lt;th&gt;Tweet&lt;/th&gt;
          &lt;th&gt;OntoNote5.0&lt;/th&gt;
          &lt;th&gt;Webpage&lt;/th&gt;
          &lt;th&gt;Wikigold&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Full Supervision&lt;/td&gt;
          &lt;td&gt;91.21&lt;/td&gt;
          &lt;td&gt;52.19&lt;/td&gt;
          &lt;td&gt;86.20&lt;/td&gt;
          &lt;td&gt;72.39&lt;/td&gt;
          &lt;td&gt;86.43&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Previous SOTA&lt;/td&gt;
          &lt;td&gt;76.00&lt;/td&gt;
          &lt;td&gt;26.10&lt;/td&gt;
          &lt;td&gt;67.69&lt;/td&gt;
          &lt;td&gt;51.39&lt;/td&gt;
          &lt;td&gt;47.54&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;BOND&lt;/td&gt;
          &lt;td&gt;81.48&lt;/td&gt;
          &lt;td&gt;48.01&lt;/td&gt;
          &lt;td&gt;68.35&lt;/td&gt;
          &lt;td&gt;65.74&lt;/td&gt;
          &lt;td&gt;60.07&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Full Supervision&lt;/em&gt;: Roberta Finetuning/BiLSTM CRF&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution)&lt;/span&gt;





 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent work on distantly supervised NER is accepted by KDD2020.
We are closing the GAP between full supervision and no supervision!
See our code here: &lt;a href=&#34;https://github.com/cliang1453/BOND&#34;&gt;https://github.com/cliang1453/BOND&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts (F1 score) are summerized as follows:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Method&lt;/th&gt;
          &lt;th&gt;CoNLL03&lt;/th&gt;
          &lt;th&gt;Tweet&lt;/th&gt;
          &lt;th&gt;OntoNote5.0&lt;/th&gt;
          &lt;th&gt;Webpage&lt;/th&gt;
          &lt;th&gt;Wikigold&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Full Supervision&lt;/td&gt;
          &lt;td&gt;91.21&lt;/td&gt;
          &lt;td&gt;52.19&lt;/td&gt;
          &lt;td&gt;86.20&lt;/td&gt;
          &lt;td&gt;72.39&lt;/td&gt;
          &lt;td&gt;86.43&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Previous SOTA&lt;/td&gt;
          &lt;td&gt;76.00&lt;/td&gt;
          &lt;td&gt;26.10&lt;/td&gt;
          &lt;td&gt;67.69&lt;/td&gt;
          &lt;td&gt;51.39&lt;/td&gt;
          &lt;td&gt;47.54&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;BOND&lt;/td&gt;
          &lt;td&gt;81.48&lt;/td&gt;
          &lt;td&gt;48.01&lt;/td&gt;
          &lt;td&gt;68.35&lt;/td&gt;
          &lt;td&gt;65.74&lt;/td&gt;
          &lt;td&gt;60.07&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Full Supervision&lt;/em&gt;: Roberta Finetuning/BiLSTM CRF&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution)&lt;/span&gt;





 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Two Papers are Accepted by ACL 2020</title>
      <link>http://localhost:1313/news/acl2020/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/news/acl2020/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ACL 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about multidomain NMT using word level mixing model.&lt;/p&gt;
&lt;p&gt;The other one is &lt;strong&gt;SMART&lt;/strong&gt; model, leaderboad wining model on GLUE.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.02692.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ACL 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about multidomain NMT using word level mixing model.&lt;/p&gt;
&lt;p&gt;The other one is &lt;strong&gt;SMART&lt;/strong&gt; model, leaderboad wining model on GLUE.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.02692.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>State-of-the-art Results on GLUE Benchmark</title>
      <link>http://localhost:1313/news/smart/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/news/smart/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (&lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;code&lt;/a&gt;) achieves state-of-the-art results in 5 of 9 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;GLUE benchmark tasks&lt;/a&gt; and an overall GLUE task performance &lt;strong&gt;89.9&lt;/strong&gt;, which outperforms all existing models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts are summerized as follows:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Method&lt;/th&gt;
          &lt;th&gt;CoLA&lt;/th&gt;
          &lt;th&gt;SST&lt;/th&gt;
          &lt;th&gt;MRPC&lt;/th&gt;
          &lt;th&gt;STS-B&lt;/th&gt;
          &lt;th&gt;QQP&lt;/th&gt;
          &lt;th&gt;MNLI-m/mm&lt;/th&gt;
          &lt;th&gt;QNLI&lt;/th&gt;
          &lt;th&gt;RTE&lt;/th&gt;
          &lt;th&gt;WNLI&lt;/th&gt;
          &lt;th&gt;AX&lt;/th&gt;
          &lt;th&gt;Score&lt;/th&gt;
          &lt;th&gt;#params&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Previous SOTA&lt;/td&gt;
          &lt;td&gt;70.8&lt;/td&gt;
          &lt;td&gt;97.1&lt;/td&gt;
          &lt;td&gt;91.9/89.2&lt;/td&gt;
          &lt;td&gt;92.5/92.1&lt;/td&gt;
          &lt;td&gt;74.6/90.4&lt;/td&gt;
          &lt;td&gt;92.0/91.7&lt;/td&gt;
          &lt;td&gt;96.7&lt;/td&gt;
          &lt;td&gt;92.5&lt;/td&gt;
          &lt;td&gt;93.2&lt;/td&gt;
          &lt;td&gt;53.1&lt;/td&gt;
          &lt;td&gt;89.7&lt;/td&gt;
          &lt;td&gt;11,000M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MT-DNN-SMART&lt;/td&gt;
          &lt;td&gt;69.5&lt;/td&gt;
          &lt;td&gt;97.5&lt;/td&gt;
          &lt;td&gt;93.7/91.6&lt;/td&gt;
          &lt;td&gt;92.9/92.5&lt;/td&gt;
          &lt;td&gt;73.9/90.2&lt;/td&gt;
          &lt;td&gt;91.0/90.8&lt;/td&gt;
          &lt;td&gt;99.2&lt;/td&gt;
          &lt;td&gt;89.7&lt;/td&gt;
          &lt;td&gt;94.5&lt;/td&gt;
          &lt;td&gt;50.2&lt;/td&gt;
          &lt;td&gt;89.9&lt;/td&gt;
          &lt;td&gt;356M&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: T5&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (&lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;code&lt;/a&gt;) achieves state-of-the-art results in 5 of 9 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;GLUE benchmark tasks&lt;/a&gt; and an overall GLUE task performance &lt;strong&gt;89.9&lt;/strong&gt;, which outperforms all existing models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts are summerized as follows:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Method&lt;/th&gt;
          &lt;th&gt;CoLA&lt;/th&gt;
          &lt;th&gt;SST&lt;/th&gt;
          &lt;th&gt;MRPC&lt;/th&gt;
          &lt;th&gt;STS-B&lt;/th&gt;
          &lt;th&gt;QQP&lt;/th&gt;
          &lt;th&gt;MNLI-m/mm&lt;/th&gt;
          &lt;th&gt;QNLI&lt;/th&gt;
          &lt;th&gt;RTE&lt;/th&gt;
          &lt;th&gt;WNLI&lt;/th&gt;
          &lt;th&gt;AX&lt;/th&gt;
          &lt;th&gt;Score&lt;/th&gt;
          &lt;th&gt;#params&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Previous SOTA&lt;/td&gt;
          &lt;td&gt;70.8&lt;/td&gt;
          &lt;td&gt;97.1&lt;/td&gt;
          &lt;td&gt;91.9/89.2&lt;/td&gt;
          &lt;td&gt;92.5/92.1&lt;/td&gt;
          &lt;td&gt;74.6/90.4&lt;/td&gt;
          &lt;td&gt;92.0/91.7&lt;/td&gt;
          &lt;td&gt;96.7&lt;/td&gt;
          &lt;td&gt;92.5&lt;/td&gt;
          &lt;td&gt;93.2&lt;/td&gt;
          &lt;td&gt;53.1&lt;/td&gt;
          &lt;td&gt;89.7&lt;/td&gt;
          &lt;td&gt;11,000M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MT-DNN-SMART&lt;/td&gt;
          &lt;td&gt;69.5&lt;/td&gt;
          &lt;td&gt;97.5&lt;/td&gt;
          &lt;td&gt;93.7/91.6&lt;/td&gt;
          &lt;td&gt;92.9/92.5&lt;/td&gt;
          &lt;td&gt;73.9/90.2&lt;/td&gt;
          &lt;td&gt;91.0/90.8&lt;/td&gt;
          &lt;td&gt;99.2&lt;/td&gt;
          &lt;td&gt;89.7&lt;/td&gt;
          &lt;td&gt;94.5&lt;/td&gt;
          &lt;td&gt;50.2&lt;/td&gt;
          &lt;td&gt;89.9&lt;/td&gt;
          &lt;td&gt;356M&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: T5&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020


&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
