<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bert on Haoming Jiang</title>
    <link>https://hmjianggatech.github.io/tags/bert/</link>
    <description>Recent content in bert on Haoming Jiang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 May 2020 22:54:07 -0400</lastBuildDate><atom:link href="https://hmjianggatech.github.io/tags/bert/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BOND is Accepted by KDD 2020</title>
      <link>https://hmjianggatech.github.io/news/bond/</link>
      <pubDate>Fri, 15 May 2020 22:54:07 -0400</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/bond/</guid>
      <description> Our recent work on distantly supervised NER is accepted by KDD2020. We are closing the GAP between full supervision and no supervision! See our code here: https://github.com/cliang1453/BOND
The reuslts (F1 score) are summerized as follows:
Method CoNLL03 Tweet OntoNote5.0 Webpage Wikigold Full Supervision 91.21 52.19 86.20 72.39 86.43 Previous SOTA 76.00 26.10 67.69 51.39 47.54 BOND 81.48 48.01 68.35 65.74 60.07 Full Supervision: Roberta Finetuning/BiLSTM CRF Previous SOTA: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET Reference BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision 2020 Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution) The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2020 </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent work on distantly supervised NER is accepted by KDD2020.
We are closing the GAP between full supervision and no supervision!
See our code here: &lt;a href=&#34;https://github.com/cliang1453/BOND&#34;&gt;https://github.com/cliang1453/BOND&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts (F1 score) are summerized as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;CoNLL03&lt;/th&gt;
&lt;th&gt;Tweet&lt;/th&gt;
&lt;th&gt;OntoNote5.0&lt;/th&gt;
&lt;th&gt;Webpage&lt;/th&gt;
&lt;th&gt;Wikigold&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Full Supervision&lt;/td&gt;
&lt;td&gt;91.21&lt;/td&gt;
&lt;td&gt;52.19&lt;/td&gt;
&lt;td&gt;86.20&lt;/td&gt;
&lt;td&gt;72.39&lt;/td&gt;
&lt;td&gt;86.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Previous SOTA&lt;/td&gt;
&lt;td&gt;76.00&lt;/td&gt;
&lt;td&gt;26.10&lt;/td&gt;
&lt;td&gt;67.69&lt;/td&gt;
&lt;td&gt;51.39&lt;/td&gt;
&lt;td&gt;47.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BOND&lt;/td&gt;
&lt;td&gt;81.48&lt;/td&gt;
&lt;td&gt;48.01&lt;/td&gt;
&lt;td&gt;68.35&lt;/td&gt;
&lt;td&gt;65.74&lt;/td&gt;
&lt;td&gt;60.07&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Full Supervision&lt;/em&gt;: Roberta Finetuning/BiLSTM CRF&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: BiLSTM-CRF/AutoNER/LR-CRF/KALM/CONNET&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Chen Liang*, Yue Yu*, Haoming Jiang*, Siawpeng Er, Ruijia Wang, Tuo Zhao and Chao Zhang (* Equal Contribution)&lt;/span&gt;





 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;The 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Two Papers are Accepted by ACL 2020</title>
      <link>https://hmjianggatech.github.io/news/acl2020/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/acl2020/</guid>
      <description> Two papers are accepted by ACL 2020!
One is about multidomain NMT using word level mixing model.
The other one is SMART model, leaderboad wining model on GLUE.
Reference SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization 2020 Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao [arXiv] [Code] Annual Conference of the Association for Computational Linguistics (ACL), 2020 Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing 2020 Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao [arXiv] Annual Conference of the Association for Computational Linguistics (ACL), 2020 </description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Two papers are accepted by ACL 2020!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One is about multidomain NMT using word level mixing model.&lt;/p&gt;
&lt;p&gt;The other one is &lt;strong&gt;SMART&lt;/strong&gt; model, leaderboad wining model on GLUE.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.02692.pdf&#34;&gt;[arXiv]&lt;/a&gt;




 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>State-of-the-art Results on GLUE Benchmark</title>
      <link>https://hmjianggatech.github.io/news/smart/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hmjianggatech.github.io/news/smart/</guid>
      <description>Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (paper, code) achieves state-of-the-art results in 5 of 9 GLUE benchmark tasks and an overall GLUE task performance 89.9, which outperforms all existing models.
The reuslts are summerized as follows:
Method CoLA SST MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score #params Previous SOTA 70.8 97.1 91.9/89.2 92.5/92.1 74.6/90.4 92.0/91.7 96.7 92.5 93.2 53.1 89.7 11,000M MT-DNN-SMART 69.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Our recent collabrative work with Microsoft Dynamics 365 AI and Microsoft Research AI (&lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;code&lt;/a&gt;) achieves state-of-the-art results in 5 of 9 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;GLUE benchmark tasks&lt;/a&gt; and an overall GLUE task performance &lt;strong&gt;89.9&lt;/strong&gt;, which outperforms all existing models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reuslts are summerized as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;CoLA&lt;/th&gt;
&lt;th&gt;SST&lt;/th&gt;
&lt;th&gt;MRPC&lt;/th&gt;
&lt;th&gt;STS-B&lt;/th&gt;
&lt;th&gt;QQP&lt;/th&gt;
&lt;th&gt;MNLI-m/mm&lt;/th&gt;
&lt;th&gt;QNLI&lt;/th&gt;
&lt;th&gt;RTE&lt;/th&gt;
&lt;th&gt;WNLI&lt;/th&gt;
&lt;th&gt;AX&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;#params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Previous SOTA&lt;/td&gt;
&lt;td&gt;70.8&lt;/td&gt;
&lt;td&gt;97.1&lt;/td&gt;
&lt;td&gt;91.9/89.2&lt;/td&gt;
&lt;td&gt;92.5/92.1&lt;/td&gt;
&lt;td&gt;74.6/90.4&lt;/td&gt;
&lt;td&gt;92.0/91.7&lt;/td&gt;
&lt;td&gt;96.7&lt;/td&gt;
&lt;td&gt;92.5&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;53.1&lt;/td&gt;
&lt;td&gt;89.7&lt;/td&gt;
&lt;td&gt;11,000M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MT-DNN-SMART&lt;/td&gt;
&lt;td&gt;69.5&lt;/td&gt;
&lt;td&gt;97.5&lt;/td&gt;
&lt;td&gt;93.7/91.6&lt;/td&gt;
&lt;td&gt;92.9/92.5&lt;/td&gt;
&lt;td&gt;73.9/90.2&lt;/td&gt;
&lt;td&gt;91.0/90.8&lt;/td&gt;
&lt;td&gt;99.2&lt;/td&gt;
&lt;td&gt;89.7&lt;/td&gt;
&lt;td&gt;94.5&lt;/td&gt;
&lt;td&gt;50.2&lt;/td&gt;
&lt;td&gt;89.9&lt;/td&gt;
&lt;td&gt;356M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Previous SOTA&lt;/em&gt;: T5&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;
&lt;span style=&#34;color:white;font-weight:bold;&#34;&gt;SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization&lt;/span&gt; &lt;span style=&#34;float:right;font-weight:bold;&#34;&gt;2020&lt;/span&gt;&lt;br/&gt;
&lt;span style=&#34;color:LightGrey&#34;&gt;Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao&lt;/span&gt;
 &lt;a href=&#34;https://arxiv.org/pdf/1911.03437.pdf&#34;&gt;[arXiv]&lt;/a&gt;

 &lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;[Code]&lt;/a&gt;


 &lt;br/&gt; &lt;strong&gt;&lt;em style=&#34;color:LightGrey&#34;&gt;Annual Conference of the Association for Computational Linguistics (ACL)&lt;/em&gt;&lt;/strong&gt;, 2020

&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
